# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>
#
# SPDX-License-Identifier: Apache-2.0

# pylint: disable=protected-access

from typing import Dict, Any, List
import json
import logging
from pathlib import Path
from collections import OrderedDict

from canals.component.component import component
from canals.pipeline.pipeline import Pipeline
from canals.errors import PipelineUnmarshalError


logger = logging.getLogger(__name__)


def save_pipelines(
    pipelines: Dict[str, Pipeline], path: Path, _writer=lambda obj, handle: json.dump(obj, handle, indent=4)
) -> None:
    """
    Converts a dictionary of named Pipelines into a JSON file.

    Args:
        pipelines: dictionary of {name: pipeline_object}
        path: where to write the resulting file
        _writer: which function to use to write the dictionary to a file.
            Use this parameter to dump to a different format like YAML, TOML, HCL, etc.

    Returns:
        None
    """
    schema = marshal_pipelines(pipelines=pipelines)
    with open(path, "w", encoding="utf-8") as handle:
        _writer(schema, handle)


def load_pipelines(path: Path, _reader=json.load) -> Dict[str, Pipeline]:
    """
    Loads the content of a JSON file generated by `save_pipelines()` into
    a dictionary of named Pipelines.

    Args:
        path: where to read the file from
        _reader: which function to use to read the dictionary to a file.
            Use this parameter to load from a different format like YAML, TOML, HCL, etc.

    Returns:
        The pipelines as a dictionary of `{"pipeline-name": <pipeline object>}`
    """
    with open(path, "r", encoding="utf-8") as handle:
        schema = _reader(handle)
    return unmarshal_pipelines(schema=schema)


def marshal_pipelines(pipelines: Dict[str, Pipeline]) -> Dict[str, Any]:
    """
    Converts a dictionary of named Pipelines into a Python dictionary that can be
    written to a JSON file.

    Args:
        pipelines: A dictionary of `{"pipeline-name": <pipeline object>}`

    Returns:
        A Python dictionary representing the Pipelines objects above, that can be written to JSON and can be reused to
        recreate the original Pipelines.
    """
    schema = {}
    schema["pipelines"] = {name: p.to_dict() for name, p in pipelines.items()}

    # Mapping of each name to a list of instance hashes.
    # This let us know if a name refers to different components.
    name_to_hashes: Dict[str, List[int]] = {}
    # Mapping of each component hash to its component.
    # Not strictly necessary but makes it faster to get a component.
    hash_to_component: Dict[int, Dict[str, Any]] = {}

    for pipeline in schema["pipelines"].values():
        for comp_name, comp_data in OrderedDict(pipeline["components"]).items():
            if comp_name not in name_to_hashes:
                name_to_hashes[comp_name] = []
            hash_ = comp_data["hash"]
            if hash_ not in name_to_hashes[comp_name]:
                name_to_hashes[comp_name].append(hash_)

            hash_to_component[hash_] = comp_data

    connections_renames = {}
    schema["components"] = {}

    for name, hashes in name_to_hashes.items():
        if len(hashes) > 1:
            # Multiple different instances with this name, we rename them
            for index, hash_ in enumerate(hashes):
                new_name = f"{name}_{index+1}"
                connections_renames[new_name] = name
                schema["components"][new_name] = hash_to_component[hash_]
            continue

        hash_ = list(hashes)[0]
        schema["components"][name] = hash_to_component[hash_]

    _rename_connections(schema, connections_renames)
    _remove_duplicate_instances(schema["components"])
    _cleanup_marshalled_data(schema)

    return schema


def _rename_connections(data: Dict[str, Any], renames: Dict[str, str]):
    """
    Rename all connections of all Pipelines found in data using renames.
    """
    for new_name, old_name in renames.items():
        for pipe in data["pipelines"].values():
            if old_name not in pipe["components"]:
                # The component to rename is not in this pipeline
                continue

            if pipe["components"][old_name]["hash"] != data["components"][new_name]["hash"]:
                # The name matches but it's another instance, it will be renamed to
                # some other name
                continue

            for connection in pipe["connections"]:
                # We split from the right just in case there is a dot in the
                # component name, socket names shouldn't contain names so it's
                # less risky this way.
                sender = connection["sender"].rsplit(".", maxsplit=1)
                receiver = connection["receiver"].rsplit(".", maxsplit=1)
                if sender[0] == old_name:
                    sender[0] = sender[0].replace(old_name, new_name)
                if receiver[0] == old_name:
                    receiver[0] = receiver[0].replace(old_name, new_name)
                connection["sender"] = ".".join(sender)
                connection["receiver"] = ".".join(receiver)


def _remove_duplicate_instances(components: Dict[str, Any]):
    """
    Remove duplicate declaration of the same component instance.
    If two or more components have the same hash remove all duplicates
    and point them to a single component.

    A structure like this:
    ```
    {
        "comp1": {"hash": 123, ...},
        "comp2": {"hash": 123, ...},
        "comp3": {"hash": 456, ...}
    }
    ```

    will become:
    ```
    {
        "comp1": {"hash": 123, ...},
        "comp2": "comp1",
        "comp3": {"hash": 456, ...}
    }
    ```
    """
    for comp_name, comp in components.items():
        if isinstance(comp, str):
            # This component is just a pointer to another one
            continue
        for other_comp_name, other_comp in components.items():
            if other_comp_name == comp_name:
                # It's the same component, skip it
                continue
            if isinstance(other_comp, str):
                # This component is just a pointer to another one
                continue
            if other_comp["hash"] == comp["hash"]:
                components[other_comp_name] = comp_name


def _cleanup_marshalled_data(schema: Dict[str, Any]):
    # Delete components declared in each pipeline, connections are enough
    # since we're declaring components globally
    for pipe in schema["pipelines"].values():
        del pipe["components"]

    # Delete components hashes, we don't need them anymore
    for comp in schema["components"].values():
        if isinstance(comp, dict):
            del comp["hash"]


def unmarshal_pipelines(schema: Dict[str, Any]) -> Dict[str, Pipeline]:  # pylint: disable=too-many-locals
    """
    Loads the content of a schema generated by `marshal_pipelines()` into
    a dictionary of named Pipelines.

    Args:
        schema: the schema of the pipelines, as generated by `marshal_pipelines()`.

    Returns:
        The pipelines as a dictionary of `{"pipeline-name": <pipeline object>}`.

    Raises PipelineUnmarshalError: if any Component class has not been imported before loading.

    """
    pipelines = {}
    component_instances: Dict[str, Any] = {}
    for pipeline_name, pipeline_schema in schema["pipelines"].items():
        # Create the Pipeline object
        pipe_args = {"metadata": pipeline_schema.get("metadata", None)}
        if "max_loops_allowed" in pipeline_schema.keys():
            pipe_args["max_loops_allowed"] = pipeline_schema["max_loops_allowed"]
        pipe = Pipeline(**pipe_args)

        # Create the components or fish them from the buffer
        for component_name, component_schema in pipeline_schema["components"].items():
            if "refer_to" in component_schema.keys():
                pipe.add_component(
                    name=component_name,
                    instance=component_instances[component_schema["refer_to"]],
                )
            else:
                class_name = component_schema["type"]
                if class_name not in component.registry:
                    raise PipelineUnmarshalError(
                        f"Failed loading Pipeline '{pipeline_name}'. Can't find Component class '{class_name}'. "
                        "Make sure you imported this class before loading the pipelines."
                    )
                component_class = component.registry[class_name]
                component_instance = component_class(**component_schema.get("init_parameters", {}))
                component_instances[f"{pipeline_name}.{component_name}"] = component_instance
                pipe.add_component(
                    name=component_name,
                    instance=component_instance,
                )

        for connect_from, connect_to, sockets in pipeline_schema["connections"]:
            output_socket, input_socket = sockets.split("/", maxsplit=1)
            connect_from = f"{connect_from}.{output_socket}"
            connect_to = f"{connect_to}.{input_socket}"
            pipe.connect(connect_from=connect_from, connect_to=connect_to)

        pipelines[pipeline_name] = pipe

    return pipelines
