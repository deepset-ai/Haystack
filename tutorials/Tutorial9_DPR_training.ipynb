{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Your Own \"Dense Passage Retrieval\" Model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.ipynb)\n",
    "\n",
    "Haystack contains all the tools needed to train your own Dense Passage Retrieval model.\n",
    "This tutorial will guide you through the steps required to create a retriever that is specifically tailored to your domain.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2021 15:03:47 - INFO - faiss -   Loading faiss with AVX2 support.\n",
      "01/07/2021 15:03:47 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "# Here are some imports that we'll need\n",
    "\n",
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "from haystack.preprocessor.utils import fetch_archive_from_http"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data\n",
    "\n",
    "DPR training performed using Information Retrieval data.\n",
    "More specifically, you want to feed in pairs of queries and relevant documents.\n",
    "\n",
    "To train a model, we will need a dataset that has the same format as the original DPR training data.\n",
    "Each data point in the dataset should have the following dictionary structure.\n",
    "\n",
    "``` python\n",
    "    {\n",
    "        \"dataset\": str,\n",
    "        \"question\": str,\n",
    "        \"answers\": list of str\n",
    "        \"positive_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
    "        \"negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
    "        \"hard_negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
    "    }\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Question Answering Data\n",
    "\n",
    "Question Answering datasets can sometimes be used as training data.\n",
    "Google's Natural Questions dataset, is sufficiently large\n",
    "and contains enough unique passages, that it can be converted into a DPR training set.\n",
    "This is done simply by considering answer containing passages as relevant documents to the query.\n",
    "\n",
    "The SQuAD dataset, however, is not as suited to this use case since its question and answer pairs\n",
    "are created on only a very small slice of wikipedia documents."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Original DPR Training Data\n",
    "\n",
    "WARNING: These files are large! The train set is 7.4GB and the dev set is 800MB\n",
    "\n",
    "We can download the original DPR training data with the following cell.\n",
    "Note that this data is probably only useful if you are trying to train from scratch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download original DPR data\n",
    "# WARNING: the train set is 7.4GB and the dev set is 800MB\n",
    "\n",
    "doc_dir = \"data/dpr_training/\"\n",
    "\n",
    "s3_url_train = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\"\n",
    "s3_url_dev = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\"\n",
    "\n",
    "fetch_archive_from_http(s3_url_train, output_dir=doc_dir + \"train/\")\n",
    "fetch_archive_from_http(s3_url_dev, output_dir=doc_dir + \"dev/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 1: Training DPR from Scratch\n",
    "\n",
    "The default variables that we provide below are chosen to train a DPR model from scratch.\n",
    "Here, both passage and query embedding models are initialized using BERT base\n",
    "and the model is trained using Google's Natural Questions dataset (in a format specialised for DPR).\n",
    "\n",
    "If you are working in a language other than English,\n",
    "you will want to initialize the passage and query embedding models with a language model that supports your language\n",
    "and also provide a dataset in your language."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Here are the variables to specify our training data, the models that we use to initialize DPR\n",
    "# and the directory where we'll be saving the model\n",
    "\n",
    "doc_dir = \"data/dpr_training/\"\n",
    "\n",
    "train_filename = \"train/biencoder-nq-train.json\"\n",
    "dev_filename = \"dev/biencoder-nq-dev.json\"\n",
    "\n",
    "query_model = \"bert-base-uncased\"\n",
    "passage_model = \"bert-base-uncased\"\n",
    "\n",
    "save_dir = \"../saved_models/dpr\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2: Finetuning DPR\n",
    "\n",
    "If you have your own domain specific question answering or information retrieval dataset,\n",
    "you might instead be interested in finetuning a pretrained DPR model.\n",
    "In this case, you would initialize both query and passage models using the original pretrained model.\n",
    "You will want to load something like this set of variables instead of the ones above"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here are the variables you might want to use instead of the set above\n",
    "# in order to perform pretraining\n",
    "\n",
    "doc_dir = \"PATH_TO_YOUR_DATA_DIR\"\n",
    "train_filename = \"TRAIN_FILENAME\"\n",
    "dev_filename = \"DEV_FILENAME\"\n",
    "\n",
    "query_model = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "passage_model = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "\n",
    "save_dir = \"../saved_models/dpr\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization\n",
    "\n",
    "Here we want to initialize our model either with plain language model weights for training from scratch\n",
    "or else with pretrained DPR weights for finetuning.\n",
    "We follow the [original DPR parameters](https://github.com/facebookresearch/DPR#best-hyperparameter-settings)\n",
    "for their max passage length but set max query length to 64 since queries are very rarely longer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2021 15:01:44 - WARNING - haystack.retriever.dense -   DensePassageRetriever initialized without a document store. This is fine if you are performing DPR training. Otherwise, please provide a document store in the constructor.\n",
      "01/07/2021 15:01:52 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "01/07/2021 15:02:00 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n"
     ]
    }
   ],
   "source": [
    "## Initialize DPR model\n",
    "\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=None,\n",
    "    query_embedding_model=query_model,\n",
    "    passage_embedding_model=passage_model,\n",
    "    max_seq_len_query=64,\n",
    "    max_seq_len_passage=256\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Let's start training and save our trained model!\n",
    "\n",
    "On a V100 GPU, you can fit up to batch size 4 so we set gradient accumulation steps to 4 in order\n",
    "to simulate the batch size 16 of the original DPR experiment.\n",
    "\n",
    "When `embed_title=True`, the document title is prepended to the input text sequence with a `[SEP]` token\n",
    "between it and document text."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When training from scratch with the above variables, 1 epoch takes around an hour and we reached the following performance:\n",
    "\n",
    "```\n",
    "loss: 0.09334952129693501\n",
    "acc: 0.984035000191887\n",
    "f1: 0.936147352264006\n",
    "acc_and_f1: 0.9600911762279465\n",
    "average_rank: 0.07075978511128166\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Dataset data/dpr_training/dev/biencoder-nq-dev.json: 100%|██████████| 6515/6515 [00:07<00:00, 928.71 Dicts/s] \n",
      "Preprocessing Dataset data/dpr_training/dev/biencoder-nq-dev.json:  66%|██████▌   | 4301/6515 [00:05<00:02, 786.28 Dicts/s] Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-11:\n",
      "Preprocessing Dataset data/dpr_training/dev/biencoder-nq-dev.json:  66%|██████▌   | 4301/6515 [00:05<00:02, 814.99 Dicts/s]Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2078, in _dict_to_samples\n",
      "    return_token_type_ids=True\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2078, in _dict_to_samples\n",
      "    return_token_type_ids=True\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2019, in _dict_to_samples\n",
      "    return_token_type_ids=True,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2144, in batch_encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2087, in _dict_to_samples\n",
      "    tokenized_passage = [self.passage_tokenizer.convert_ids_to_tokens(ctx) for ctx in ctx_input_ids]\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 390, in _batch_encode_plus\n",
      "    is_pretokenized=is_split_into_words,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2144, in batch_encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 249, in encode_batch\n",
      "    return self._tokenizer.encode_batch(inputs, is_pretokenized, add_special_tokens)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 390, in _batch_encode_plus\n",
      "    is_pretokenized=is_split_into_words,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/data_silo.py\", line 131, in _dataset_from_chunk\n",
      "    dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 249, in encode_batch\n",
      "    return self._tokenizer.encode_batch(inputs, is_pretokenized, add_special_tokens)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 415, in dataset_from_dicts\n",
      "    self._init_samples_in_baskets()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2087, in <listcomp>\n",
      "    tokenized_passage = [self.passage_tokenizer.convert_ids_to_tokens(ctx) for ctx in ctx_input_ids]\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2050, in encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 473, in _encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 255, in convert_ids_to_tokens\n",
      "    tokens.append(self._tokenizer.id_to_token(index))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 366, in _batch_encode_plus\n",
      "    pad_to_multiple_of=pad_to_multiple_of,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 311, in id_to_token\n",
      "    return self._tokenizer.id_to_token(id)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 317, in set_truncation_and_padding\n",
      "    pad_to_multiple_of=pad_to_multiple_of,\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 91, in enable_padding\n",
      "    length=length,\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 341, in _init_samples_in_baskets\n",
      "    basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2087, in _dict_to_samples\n",
      "    tokenized_passage = [self.passage_tokenizer.convert_ids_to_tokens(ctx) for ctx in ctx_input_ids]\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2087, in <listcomp>\n",
      "    tokenized_passage = [self.passage_tokenizer.convert_ids_to_tokens(ctx) for ctx in ctx_input_ids]\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/data_handler/processor.py\", line 2019, in _dict_to_samples\n",
      "    return_token_type_ids=True,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 255, in convert_ids_to_tokens\n",
      "    tokens.append(self._tokenizer.id_to_token(index))\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2050, in encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 311, in id_to_token\n",
      "    return self._tokenizer.id_to_token(id)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\", line 484, in _encode_plus\n",
      "    batched_output.encodings,\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 179, in __init__\n",
      "    super().__init__(data)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/collections/__init__.py\", line 1018, in __init__\n",
      "    self.update(dict)\n",
      "  File \"/home/branden/Code/anaconda3/envs/haystack/lib/python3.7/_collections_abc.py\", line 824, in update\n",
      "    def update(*args, **kwds):\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training our model and save it when it is finished\n",
    "\n",
    "retriever.train(\n",
    "    data_dir=doc_dir,\n",
    "    train_filename=dev_filename,\n",
    "    dev_filename=dev_filename,\n",
    "    test_filename=dev_filename,\n",
    "    n_epochs=1,\n",
    "    batch_size=4,\n",
    "    grad_acc_steps=4,\n",
    "    save_dir=save_dir,\n",
    "    evaluate_every=3000,\n",
    "    embed_title=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading\n",
    "\n",
    "Loading our newly trained model is simple!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2021 15:08:21 - WARNING - haystack.retriever.dense -   DensePassageRetriever initialized without a document store. This is fine if you are performing DPR training. Otherwise, please provide a document store in the constructor.\n",
      "01/07/2021 15:08:22 - WARNING - farm.modeling.tokenization -   No config file found. Trying to infer Tokenizer type from model name\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not infer tokenizer_class from model config or name '..saved_models/dpr/query_encoder'. Set arg `tokenizer_class` in Tokenizer.load() to one of: AlbertTokenizer, XLMRobertaTokenizer, RobertaTokenizer, DistilBertTokenizer, BertTokenizer, XLNetTokenizer, CamembertTokenizer, ElectraTokenizer, DPRQuestionEncoderTokenizer,DPRContextEncoderTokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mresolved_config_file\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 359\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    360\u001B[0m             \u001B[0mconfig_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dict_from_json_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresolved_config_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/modeling/tokenization.py\u001B[0m in \u001B[0;36m_infer_tokenizer_class\u001B[0;34m(pretrained_model_name_or_path)\u001B[0m\n\u001B[1;32m    147\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 148\u001B[0;31m             \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    149\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m         \"\"\"\n\u001B[0;32m--> 310\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m             )\n\u001B[0;32m--> 368\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load config for '..saved_models/dpr/query_encoder'. Make sure that:\n\n- '..saved_models/dpr/query_encoder' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '..saved_models/dpr/query_encoder' is the correct path to a directory containing a config.json file\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mresolved_config_file\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 359\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    360\u001B[0m             \u001B[0mconfig_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dict_from_json_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresolved_config_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/modeling/tokenization.py\u001B[0m in \u001B[0;36m_infer_tokenizer_class\u001B[0;34m(pretrained_model_name_or_path)\u001B[0m\n\u001B[1;32m    151\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 152\u001B[0;31m                 \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/language_model_config.json\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    153\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m         \"\"\"\n\u001B[0;32m--> 310\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m             )\n\u001B[0;32m--> 368\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load config for '..saved_models/dpr/query_encoder/language_model_config.json'. Make sure that:\n\n- '..saved_models/dpr/query_encoder/language_model_config.json' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '..saved_models/dpr/query_encoder/language_model_config.json' is the correct path to a directory containing a config.json file\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-e04f822edfe0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mreloaded_retriever\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDensePassageRetriever\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mload_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdocument_store\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Code/haystack/haystack/retriever/dense.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(cls, load_dir, document_store, max_seq_len_query, max_seq_len_passage, use_gpu, batch_size, embed_title, use_fast_tokenizers, similarity_function, query_encoder_dir, passage_encoder_dir)\u001B[0m\n\u001B[1;32m    370\u001B[0m             \u001B[0mmax_seq_len_query\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_seq_len_query\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m             \u001B[0mmax_seq_len_passage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_seq_len_passage\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 372\u001B[0;31m             \u001B[0muse_gpu\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_gpu\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    373\u001B[0m             \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    374\u001B[0m             \u001B[0membed_title\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0membed_title\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/haystack/haystack/retriever/dense.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, document_store, query_embedding_model, passage_embedding_model, max_seq_len_query, max_seq_len_passage, use_gpu, batch_size, embed_title, use_fast_tokenizers, similarity_function)\u001B[0m\n\u001B[1;32m    106\u001B[0m         \u001B[0;31m# Init & Load Encoders\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m         self.query_tokenizer = Tokenizer.load(pretrained_model_name_or_path=query_embedding_model,\n\u001B[0;32m--> 108\u001B[0;31m                                               \u001B[0mdo_lower_case\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m                                               \u001B[0muse_fast\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_fast_tokenizers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m                                               tokenizer_class=\"DPRQuestionEncoderTokenizer\")\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/modeling/tokenization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(cls, pretrained_model_name_or_path, tokenizer_class, use_fast, **kwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtokenizer_class\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0mtokenizer_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_infer_tokenizer_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Loading tokenizer of type '{tokenizer_class}'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/modeling/tokenization.py\u001B[0m in \u001B[0;36m_infer_tokenizer_class\u001B[0;34m(pretrained_model_name_or_path)\u001B[0m\n\u001B[1;32m    153\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"No config file found. Trying to infer Tokenizer type from model name\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m                 \u001B[0mtokenizer_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_infer_tokenizer_class_from_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    156\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mtokenizer_class\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/anaconda3/envs/haystack/lib/python3.7/site-packages/farm/modeling/tokenization.py\u001B[0m in \u001B[0;36m_infer_tokenizer_class_from_string\u001B[0;34m(pretrained_model_name_or_path)\u001B[0m\n\u001B[1;32m    226\u001B[0m             \u001B[0mtokenizer_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"DPRContextEncoderTokenizer\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m             raise ValueError(f\"Could not infer tokenizer_class from model config or \"\n\u001B[0m\u001B[1;32m    229\u001B[0m                              \u001B[0;34mf\"name '{pretrained_model_name_or_path}'. Set arg `tokenizer_class` \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m                              \u001B[0;34mf\"in Tokenizer.load() to one of: AlbertTokenizer, XLMRobertaTokenizer, \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Could not infer tokenizer_class from model config or name '..saved_models/dpr/query_encoder'. Set arg `tokenizer_class` in Tokenizer.load() to one of: AlbertTokenizer, XLMRobertaTokenizer, RobertaTokenizer, DistilBertTokenizer, BertTokenizer, XLNetTokenizer, CamembertTokenizer, ElectraTokenizer, DPRQuestionEncoderTokenizer,DPRContextEncoderTokenizer."
     ]
    }
   ],
   "source": [
    "reloaded_retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "data format\n",
     "Explain what is DPR\n",
     "embed title\n",
     "initialisation\n",
     "- fine tune vs train from scratch (BERT)\n",
     "colab package installation\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}