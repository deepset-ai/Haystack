{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation via \"Retrieval-Augmented Generation\"\n",
    "\n",
    "EXECUTABLE VERSION: [colab](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial7_RAG_Generator.ipynb)\n",
    "\n",
    "\n",
    "### \"Retrieval-Augmented Generation\"\n",
    "\n",
    "In this Tutorial, we want to highlight \"Retrieval-Augmented Generation\" along with \"Dense Dual-Encoder\" called Dense Passage Retriever in Haystack codebase. \n",
    "\n",
    "Original Abstract: \n",
    "\n",
    "_\"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\"_\n",
    "\n",
    "Paper: https://arxiv.org/abs/2005.11401  \n",
    "Original Code: https://github.com/huggingface/transformers\n",
    "\n",
    "\n",
    "*Use this* [link](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial7_RAG_Generator.ipynb) *to open the notebook in Google Colab.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest release of Haystack in your own environment \n",
    "#! pip install farm-haystack\n",
    "\n",
    "# Install the latest master of Haystack\n",
    "!pip install git+https://github.com/deepset-ai/haystack/\n",
    "!pip install urllib3==1.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack.document_store.faiss import FAISSDocumentStore\n",
    "from haystack.generator.transformers import RAGenerator\n",
    "from haystack.retriever.dense import DensePassageRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents from which you want generate answers\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"The capital of Germany is the city state of Berlin.\"\"\"\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"Berlin is the capital and largest city of Germany by both area and population.\"\"\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS document store to documents and corresponding index for embeddings \n",
    "document_store = FAISSDocumentStore()\n",
    "\n",
    "# Initialize DPR Retriever to encode documents, encode question and query documents\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=False,\n",
    "    embed_title=True,\n",
    "    remove_sep_tok_from_untitled_passages=True\n",
    ")\n",
    "\n",
    "# Initialize RAG Generator\n",
    "generator = RAGenerator(\n",
    "    model_name_or_path=\"facebook/rag-token-nq\",\n",
    "    retriever=retriever,\n",
    "    use_gpu=False,\n",
    "    top_k_answers=1,\n",
    "    max_length=200,\n",
    "    min_length=2,\n",
    "    embed_title=True,\n",
    "    num_beams=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing documents in documents store\n",
    "document_store.delete_all_documents()\n",
    "# Write documents to document store\n",
    "document_store.write_documents(documents)\n",
    "# Add documents embeddings to index\n",
    "document_store.update_embeddings(\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ask your question and retrieve related documents from retriever\n",
    "question = \"What is capital of the Germany?\"\n",
    "retriever_results = retriever.retrieve(\n",
    "    query=question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate answer from question and retrieved documents\n",
    "predicted_result = generator.predict(\n",
    "    question=question,\n",
    "    documents=retriever_results,\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "# Print you answer\n",
    "answers = predicted_result[\"answers\"]\n",
    "for idx, answer in enumerate(answers):\n",
    "    print(f'Generated answer#{idx + 1} is{answer[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
