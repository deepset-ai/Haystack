{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeAkZwDhufYA"
   },
   "source": [
    "# Open-Domain QA on Tables\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial15_TableQA.ipynb)\n",
    "\n",
    "This tutorial shows you how to perform question-answering on tables using the `TableTextRetriever` or `ElasticsearchRetriever` as retriever node and the `TableReader` as reader node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbR3bETlvi-3"
   },
   "source": [
    "### Prepare environment\n",
    "\n",
    "#### Colab: Enable the GPU runtime\n",
    "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
    "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/_src/img/colab_gpu_runtime.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW66x0rfujyO"
   },
   "outputs": [],
   "source": [
    "# Make sure you have a GPU running\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZXoyhOAvn7M"
   },
   "outputs": [],
   "source": [
    "# Install the latest release of Haystack in your own environment\n",
    "#! pip install farm-haystack\n",
    "\n",
    "# Install the latest master of Haystack\n",
    "!pip install --upgrade pip\n",
    "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]\n",
    "\n",
    "# The TaPAs-based TableReader requires the torch-scatter library\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "\n",
    "# Install pygraphviz for visualization of Pipelines\n",
    "!apt install libgraphviz-dev\n",
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_XJhluXwF5_"
   },
   "source": [
    "### Start an Elasticsearch server\n",
    "You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (e.g. in Colab notebooks), then you can manually download and execute Elasticsearch from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frDqgzK7v2i1"
   },
   "outputs": [],
   "source": [
    "# Recommended: Start Elasticsearch using Docker via the Haystack utility function\n",
    "from haystack.utils import launch_es\n",
    "\n",
    "launch_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4PGj1A6wKWu"
   },
   "outputs": [],
   "source": [
    "# In Colab / No Docker environments: Start Elasticsearch from source\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.9.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "es_server = Popen(\n",
    "    [\"elasticsearch-7.9.2/bin/elasticsearch\"], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    ")\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmxepXZtwQ0E"
   },
   "outputs": [],
   "source": [
    "# Connect to Elasticsearch\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# We want to use a small model producing 512-dimensional embeddings, so we need to set embedding_dim to 512\n",
    "document_index = \"document\"\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=\"localhost\", username=\"\", password=\"\", index=document_index, embedding_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFh26LIlxldw"
   },
   "source": [
    "## Add Tables to DocumentStore\n",
    "To quickly demonstrate the capabilities of the `TableTextRetriever` and the `TableReader` we use a subset of 1000 tables and text documents from a dataset we have published in [this paper](https://arxiv.org/abs/2108.04049).\n",
    "\n",
    "Just as text passages, tables are represented as `Document` objects in Haystack. The content field, though, is a pandas DataFrame instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM63uwbd8zd6"
   },
   "outputs": [],
   "source": [
    "# Let's first fetch some tables that we want to query\n",
    "# Here: 1000 tables from OTT-QA\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "doc_dir = \"data\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/table_text_dataset.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKjw2LuXxlGh",
    "outputId": "92c67d24-d6fb-413e-8dd7-53075141d508"
   },
   "outputs": [],
   "source": [
    "# Add the tables to the DocumentStore\n",
    "\n",
    "import json\n",
    "from haystack import Document\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_tables(filename):\n",
    "    processed_tables = []\n",
    "    with open(filename) as tables:\n",
    "        tables = json.load(tables)\n",
    "        for key, table in tables.items():\n",
    "            current_columns = table[\"header\"]\n",
    "            current_rows = table[\"data\"]\n",
    "            current_df = pd.DataFrame(columns=current_columns, data=current_rows)\n",
    "            document = Document(content=current_df, content_type=\"table\", id=key)\n",
    "            processed_tables.append(document)\n",
    "\n",
    "    return processed_tables\n",
    "\n",
    "\n",
    "tables = read_tables(f\"{doc_dir}/tables.json\")\n",
    "document_store.write_documents(tables, index=document_index)\n",
    "\n",
    "# Showing content field and meta field of one of the Documents of content_type 'table'\n",
    "print(tables[0].content)\n",
    "print(tables[0].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmQC1sDmw3d7"
   },
   "source": [
    "## Initalize Retriever, Reader, & Pipeline\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Retrievers help narrowing down the scope for the Reader to a subset of tables where a given question could be answered.\n",
    "They use some simple but fast algorithm.\n",
    "\n",
    "**Here:** We use the `TableTextRetriever` capable of retrieving relevant content among a database\n",
    "of texts and tables using dense embeddings. It is an extension of the `DensePassageRetriever` and consists of three encoders (one query encoder, one text passage encoder and one table encoder) that create embeddings in the same vector space. More details on the `TableTextRetriever` and how it is trained can be found in [this paper](https://arxiv.org/abs/2108.04049).\n",
    "\n",
    "**Alternatives:**\n",
    "\n",
    "- `ElasticsearchRetriever` that uses BM25 algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY_qvdV6wyK5"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes.retriever import TableTextRetriever\n",
    "\n",
    "retriever = TableTextRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"deepset/bert-small-mm_retrieval-question_encoder\",\n",
    "    passage_embedding_model=\"deepset/bert-small-mm_retrieval-passage_encoder\",\n",
    "    table_embedding_model=\"deepset/bert-small-mm_retrieval-table_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jasi1RM2zIJ7"
   },
   "outputs": [],
   "source": [
    "# Add table embeddings to the tables in DocumentStore\n",
    "document_store.update_embeddings(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM-ijy6Zz11L"
   },
   "outputs": [],
   "source": [
    "## Alternative: ElasticsearchRetriever\n",
    "# from haystack.nodes.retriever import ElasticsearchRetriever\n",
    "# retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHfQWxVI0N2e",
    "outputId": "1d8dc4d2-a184-489e-defa-d445d76c458f"
   },
   "outputs": [],
   "source": [
    "# Try the Retriever\n",
    "from haystack.utils import print_documents\n",
    "\n",
    "retrieved_tables = retriever.retrieve(\"Who won the Super Bowl?\", top_k=5)\n",
    "# Get highest scored table\n",
    "print(retrieved_tables[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbwkXScm2-gy"
   },
   "source": [
    "### Reader\n",
    "The `TableReader` is based on TaPas, a transformer-based language model capable of grasping the two-dimensional structure of a table. It scans the tables returned by the retriever and extracts the anser. The available TableReader models can be found [here](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads).\n",
    "\n",
    "**Notice**: The `TableReader` will return an answer for each table, even if the query cannot be answered by the table. Furthermore, the confidence scores are not useful as of now, given that they will *always* be very high (i.e. 1 or close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4APcRoio2RxG"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import TableReader\n",
    "\n",
    "reader = TableReader(model_name_or_path=\"google/tapas-base-finetuned-wtq\", max_seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILuAXkyN4F7x",
    "outputId": "4bd19dcb-df8e-4a4d-b9d2-d34650e9e5c2"
   },
   "outputs": [],
   "source": [
    "# Try the TableReader on one Table (highest-scored retrieved table from previous section)\n",
    "\n",
    "table_doc = document_store.get_document_by_id(\"36964e90-3735-4ba1-8e6a-bec236e88bb2\")\n",
    "print(table_doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilbsecgA4vfN",
    "outputId": "f845f43e-43e8-48fe-d0ef-91b17a5eff0e"
   },
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "prediction = reader.predict(query=\"Who played Gregory House in the series House?\", documents=[table_doc])\n",
    "print_answers(prediction, details=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkAYNMb7R9qu"
   },
   "source": [
    "The offsets in the `offsets_in_document` and `offsets_in_context` field indicate the table cells that the model predicts to be part of the answer. They need to be interpreted on the linearized table, i.e., a flat list containing all of the table cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "It8XYT2ZTVJs",
    "outputId": "7d31af60-e04a-485d-f0ee-f29592b03928"
   },
   "outputs": [],
   "source": [
    "print(f\"Predicted answer: {prediction['answers'][0].answer}\")\n",
    "print(f\"Meta field: {prediction['answers'][0].meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgmG7pzL5ceh"
   },
   "source": [
    "### Pipeline\n",
    "The Retriever and the Reader can be sticked together to a pipeline in order to first retrieve relevant tables and then extract the answer.\n",
    "\n",
    "**Notice**: Given that the `TableReader` does not provide useful confidence scores and returns an answer for each of the tables, the sorting of the answers might be not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-aZZvyv4-Mf"
   },
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "from haystack import Pipeline\n",
    "\n",
    "table_qa_pipeline = Pipeline()\n",
    "table_qa_pipeline.add_node(component=retriever, name=\"TableTextRetriever\", inputs=[\"Query\"])\n",
    "table_qa_pipeline.add_node(component=reader, name=\"TableReader\", inputs=[\"TableTextRetriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8evexnW6dev",
    "outputId": "40514084-f516-4f13-fb48-6a55cb578366"
   },
   "outputs": [],
   "source": [
    "prediction = table_qa_pipeline.run(\"When was Guilty Gear Xrd : Sign released?\", params={\"top_k\": 30})\n",
    "print_answers(prediction, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CBcIjIq_uFx"
   },
   "outputs": [],
   "source": [
    "# Add 500 text passages to our document store.\n",
    "\n",
    "\n",
    "def read_texts(filename):\n",
    "    processed_passages = []\n",
    "    with open(filename) as passages:\n",
    "        passages = json.load(passages)\n",
    "        for key, content in passages.items():\n",
    "            document = Document(content=content, content_type=\"text\", id=key)\n",
    "            processed_passages.append(document)\n",
    "\n",
    "    return processed_passages\n",
    "\n",
    "\n",
    "passages = read_texts(f\"{doc_dir}/texts.json\")\n",
    "document_store.write_documents(passages, index=document_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1TaNF7SiKgH"
   },
   "outputs": [],
   "source": [
    "document_store.update_embeddings(retriever=retriever, update_existing_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2sk_uNHj0DY"
   },
   "source": [
    "## Pipeline for QA on Combination of Text and Tables\n",
    "We are using one node for retrieving both texts and tables, the `TableTextRetriever`. In order to do question-answering on the Documents coming from the `TableTextRetriever`, we need to route Documents of type `\"text\"` to a `FARMReader` (or alternatively `TransformersReader`) and Documents of type `\"table\"` to a `TableReader`.\n",
    "\n",
    "To achieve this, we make use of two additional nodes:\n",
    "- `SplitDocumentList`: Splits the List of Documents retrieved by the `TableTextRetriever` into two lists containing only Documents of type `\"text\"` or `\"table\"`, respectively.\n",
    "- `JoinAnswers`: Takes Answers coming from two different Readers (in this case `FARMReader` and `TableReader`) and joins them to a single list of Answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej_j8Q3wlxXE"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader, RouteDocuments, JoinAnswers\n",
    "\n",
    "text_reader = FARMReader(\"deepset/roberta-base-squad2\")\n",
    "# In order to get meaningful scores from the TableReader, use \"deepset/tapas-large-nq-hn-reader\" or\n",
    "# \"deepset/tapas-large-nq-reader\" as TableReader models. The disadvantage of these models is, however,\n",
    "# that they are not capable of doing aggregations over multiple table cells.\n",
    "table_reader = TableReader(\"deepset/tapas-large-nq-hn-reader\")\n",
    "route_documents = RouteDocuments()\n",
    "join_answers = JoinAnswers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zdq6JnF5m3aP"
   },
   "outputs": [],
   "source": [
    "text_table_qa_pipeline = Pipeline()\n",
    "text_table_qa_pipeline.add_node(component=retriever, name=\"TableTextRetriever\", inputs=[\"Query\"])\n",
    "text_table_qa_pipeline.add_node(component=route_documents, name=\"RouteDocuments\", inputs=[\"TableTextRetriever\"])\n",
    "text_table_qa_pipeline.add_node(component=text_reader, name=\"TextReader\", inputs=[\"RouteDocuments.output_1\"])\n",
    "text_table_qa_pipeline.add_node(component=table_reader, name=\"TableReader\", inputs=[\"RouteDocuments.output_2\"])\n",
    "text_table_qa_pipeline.add_node(component=join_answers, name=\"JoinAnswers\", inputs=[\"TextReader\", \"TableReader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "K4vH1ZEnniut",
    "outputId": "85aa17a8-227d-40e4-c8c0-5d0532faa47a"
   },
   "outputs": [],
   "source": [
    "# Let's have a look on the structure of the combined Table an Text QA pipeline.\n",
    "from IPython import display\n",
    "\n",
    "text_table_qa_pipeline.draw()\n",
    "display.Image(\"pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strPNduPoBLe"
   },
   "outputs": [],
   "source": [
    "# Example query whose answer resides in a text passage\n",
    "predictions = text_table_qa_pipeline.run(query=\"Who was Thomas Alva Edison?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YiK75tSoOGA",
    "outputId": "bd52f841-3846-441f-dd6f-53b02111691e"
   },
   "outputs": [],
   "source": [
    "# We can see both text passages and tables as contexts of the predicted answers.\n",
    "print_answers(predictions, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYOHDSmLpzEg"
   },
   "outputs": [],
   "source": [
    "# Example query whose answer resides in a table\n",
    "predictions = text_table_qa_pipeline.run(query=\"Which country does the film Macaroni come from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kw53uWep3zj",
    "outputId": "b332cc17-3cb8-4e20-d79d-bb4cf656f277"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-11:\n",
      "Process SpawnPoolWorker-12:\n",
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-9:\n",
      "Process SpawnPoolWorker-8:\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 366, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# We can see both text passages and tables as contexts of the predicted answers.\n",
    "print_answers(predictions, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "To evaluate our pipeline, we can use haystack's evaluation feature. We just need to convert our labels into `MultiLabel` objects and the `eval` method will do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Label, MultiLabel, Answer\n",
    "\n",
    "\n",
    "def read_labels(filename, tables):\n",
    "    processed_labels = []\n",
    "    with open(filename) as labels:\n",
    "        labels = json.load(labels)\n",
    "        for table in tables:\n",
    "            if table.id not in labels:\n",
    "                continue\n",
    "            label = labels[table.id]\n",
    "            label = Label(\n",
    "                query=label[\"query\"],\n",
    "                document=table,\n",
    "                is_correct_answer=True,\n",
    "                is_correct_document=True,\n",
    "                answer=Answer(answer=label[\"answer\"]),\n",
    "                origin=\"gold-label\",\n",
    "            )\n",
    "            processed_labels.append(MultiLabel(labels=[label]))\n",
    "    return processed_labels\n",
    "\n",
    "\n",
    "table_labels = read_labels(f\"{doc_dir}/labels.json\", tables)\n",
    "passage_labels = read_labels(f\"{doc_dir}/labels.json\", passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = text_table_qa_pipeline.eval(table_labels + passage_labels, params={\"top_k\": 10})\n",
    "print(eval_results.calculate_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding tables from PDFs\n",
    "It can sometimes be hard to provide your data in form of a pandas DataFrame. For this case, we provide the `ParsrConverter` wrapper that can help you to convert, for example, a PDF file into a document that you can index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 3001:3001 axarev/parsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.w3.org/WAI/WCAG21/working-examples/pdf-table/table.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import ParsrConverter\n",
    "\n",
    "converter = ParsrConverter()\n",
    "\n",
    "docs = converter.convert(\"table.pdf\")\n",
    "\n",
    "tables = [doc for doc in docs if doc.content_type == \"table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [doc for doc in docs if doc[\"content_type\"] == \"table\"]\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyeK3s28_X1C"
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Slack](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Tutorial15_TableQA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
